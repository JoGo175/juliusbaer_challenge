{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saji/miniconda3/envs/bert/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/Users/saji/miniconda3/envs/bert/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# import packages\n",
    "import torch\n",
    "import gc\n",
    "from transformers import  pipeline, AutoProcessor,AutoFeatureExtractor, AutoModelForAudioClassification, Wav2Vec2Processor, AutoConfig\n",
    "from huggingface_hub import notebook_login\n",
    "import librosa\n",
    "import os\n",
    "import pandas as pd\n",
    "#import torchaudio\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, df, feature_extractor):\n",
    "        self.df = df\n",
    "        self.feature_extractor = feature_extractor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.df.iloc[idx]['file_path']\n",
    "        label = self.df.iloc[idx]['label']\n",
    "        waveform, sample_rate = librosa.load(file_path, sr=16000)\n",
    "        inputs = self.feature_extractor(waveform, sampling_rate=sample_rate, return_tensors=\"pt\")\n",
    "        return inputs, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullAudioDataset(Dataset):\n",
    "    def __init__(self, df, feature_extractor):\n",
    "        self.df = df\n",
    "        self.feature_extractor = feature_extractor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.df.iloc[idx]['file_path']\n",
    "        waveform, sample_rate = librosa.load(file_path, sr=16000)\n",
    "        inputs = self.feature_extractor(waveform, sampling_rate=sample_rate, return_tensors=\"pt\")\n",
    "        file_name = self.df.iloc[idx]['file_name_without_ext']\n",
    "        return inputs, file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your directory containing the .wav files and the CSV file\n",
    "csv_file_path_fake = '/Users/saji/Desktop/juliusbaer-main/client_profiles/fake_recordings.csv'\n",
    "# Read the CSV file\n",
    "df_fake = pd.read_csv(csv_file_path_fake)\n",
    "\n",
    "# Path to your directory containing the .wav files and the CSV file\n",
    "csv_file_path_real = '/Users/saji/Desktop/juliusbaer-main/client_profiles/real_recordings.csv'\n",
    "# Read the CSV file\n",
    "df_real = pd.read_csv(csv_file_path_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_extractor = AutoFeatureExtractor.from_pretrained(\"MelodyMachine/Deepfake-audio-detection-V2\")\n",
    "#model = AutoModelForAudioClassification.from_pretrained(\"MelodyMachine/Deepfake-audio-detection-V2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saji/miniconda3/envs/bert/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"MattyB95/AST-ASVspoof2019-Synthetic-Voice-Detection-New\")\n",
    "model = AutoModelForAudioClassification.from_pretrained(\"MattyB95/AST-ASVspoof2019-Synthetic-Voice-Detection-New\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_extractor = AutoFeatureExtractor.from_pretrained(\"HyperMoon/wav2vec2-base-960h-finetuned-deepfake\")\n",
    "#model = AutoModelForAudioClassification.from_pretrained(\"HyperMoon/wav2vec2-base-960h-finetuned-deepfake\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_extractor = AutoFeatureExtractor.from_pretrained(\"abhishtagatya/hubert-base-960h-itw-deepfake\")\n",
    "#model = AutoModelForAudioClassification.from_pretrained(\"abhishtagatya/hubert-base-960h-itw-deepfake\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_extractor = AutoFeatureExtractor.from_pretrained(\"Gustking/wav2vec2-large-xlsr-deepfake-audio-classification\")\n",
    "#model = AutoModelForAudioClassification.from_pretrained(\"Gustking/wav2vec2-large-xlsr-deepfake-audio-classification\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all layers except the classifier\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "#for param in model.projector.parameters():\n",
    "#    param.requires_grad = True\n",
    "\n",
    "#optimizer = Adam(model.classifier.parameters(), lr=5e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"/Users/saji/Desktop/juliusbaer-main/notebooks/audio_data/all/\"\n",
    "\n",
    "# Create the full paths\n",
    "fake_wav_list = PATH + df_fake['rec_id'] + \".wav\"\n",
    "fake_labels = [0] * 20\n",
    "\n",
    "# Create the full paths\n",
    "real_wav_list = PATH + df_real['rec_id'] + \".wav\"\n",
    "real_labels = [1] * 20\n",
    "\n",
    "\n",
    "\n",
    "# Combine the lists\n",
    "combined_wav_list = pd.concat([fake_wav_list, real_wav_list]).tolist()\n",
    "combined_labels = fake_labels + real_labels\n",
    "\n",
    "# Create a DataFrame from the combined lists\n",
    "df_data = pd.DataFrame({\n",
    "    'file_path': combined_wav_list,\n",
    "    'label': combined_labels\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(df_data, test_size=0.6, stratify=df_data['label'], random_state=999)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AudioDataset(train_df, feature_extractor)\n",
    "val_dataset = AudioDataset(val_df, feature_extractor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Define the optimizer\n",
    "#best matty\n",
    "optimizer = Adam(model.classifier.parameters(), lr=8e-4, weight_decay=1e-3)\n",
    "\n",
    "#optimizer = Adam(model.classifier.parameters(), lr=4e-5, weight_decay=1e-3)\n",
    "\n",
    "#optimizer = Adam(list(model.classifier.parameters()) + list(model.projector.parameters()), lr=2e-4, weight_decay=1e-5)\n",
    "\n",
    "\n",
    "# Define the loss function\n",
    "criterion = CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kk/cjf23s7n4qd7bd7fc55tzfhh0000gn/T/ipykernel_69510/1727611638.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label = torch.tensor(label).to(model.device)\n",
      "/var/folders/kk/cjf23s7n4qd7bd7fc55tzfhh0000gn/T/ipykernel_69510/1727611638.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label = torch.tensor(label).to(model.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8, Training Loss: 6.4145, Validation Loss: 3.8111\n",
      "Epoch 2/8, Training Loss: 1.3764, Validation Loss: 0.7511\n",
      "Epoch 3/8, Training Loss: 1.0878, Validation Loss: 0.9358\n",
      "Epoch 4/8, Training Loss: 0.4681, Validation Loss: 0.4514\n",
      "Epoch 5/8, Training Loss: 0.5493, Validation Loss: 0.4573\n",
      "Epoch 6/8, Training Loss: 0.3924, Validation Loss: 0.3902\n",
      "Epoch 7/8, Training Loss: 0.3298, Validation Loss: 0.3398\n",
      "Epoch 8/8, Training Loss: 0.3025, Validation Loss: 0.3635\n"
     ]
    }
   ],
   "source": [
    "# Fine-tune the classifier\n",
    "model.train()\n",
    "num_epochs = 8\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_train_loss = 0.0\n",
    "    model.train()\n",
    "    for inputs, label in train_dataloader:\n",
    "        input_values = inputs[\"input_values\"].squeeze(0)  # Remove batch dimension\n",
    "        #attention_mask = inputs[\"attention_mask\"].squeeze(0)  # Remove batch dimension\n",
    "\n",
    "        # Move tensors to the appropriate device\n",
    "        input_values = input_values.to(model.device)\n",
    "        #attention_mask = attention_mask.to(model.device)\n",
    "        label = torch.tensor(label).to(model.device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_values=input_values)\n",
    "        logits = outputs.logits\n",
    "        #print(logits)\n",
    "        #print(label)\n",
    "        loss = criterion(logits, label)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "\n",
    "    # Validate the model\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, label in val_dataloader:\n",
    "            input_values = inputs[\"input_values\"].squeeze(0)  # Remove batch dimension\n",
    "            #attention_mask = inputs[\"attention_mask\"].squeeze(0)  # Remove batch dimension\n",
    "\n",
    "            # Move tensors to the appropriate device\n",
    "            input_values = input_values.to(model.device)\n",
    "            #attention_mask = attention_mask.to(model.device)\n",
    "            label = torch.tensor(label).to(model.device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_values=input_values)\n",
    "            logits = outputs.logits\n",
    "            loss = criterion(logits, label)\n",
    "\n",
    "            total_val_loss += loss.item()\n",
    "    \n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    # Save the model after each epoch\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': avg_val_loss,\n",
    "    }, f'model_epoch_{epoch + 1}.pth')\n",
    "\n",
    "    # Save the best model based on validation loss\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_val_loss,\n",
    "        }, 'best_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kk/cjf23s7n4qd7bd7fc55tzfhh0000gn/T/ipykernel_60118/2838972850.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(model.device)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "# Initialize lists to hold the data\n",
    "predicted_labels_list = []\n",
    "probabilities_list = []\n",
    "true_labels_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_dataloader:\n",
    "        input_values = inputs[\"input_values\"].squeeze(0)  # Remove batch dimension\n",
    "        # Move tensor to the appropriate device\n",
    "        input_values = input_values.to(model.device)\n",
    "        labels = torch.tensor(labels).to(model.device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_values=input_values)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Apply softmax to convert logits into probabilities\n",
    "        probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "        # Get the predicted labels\n",
    "        _, predicted_labels = torch.max(probabilities, dim=-1)\n",
    "\n",
    "        # Convert tensors to list and store in the lists\n",
    "        predicted_labels_list.extend(predicted_labels.cpu().numpy())\n",
    "        probabilities_list.extend(probabilities.cpu().numpy())\n",
    "        true_labels_list.extend(labels.cpu().numpy())\n",
    "\n",
    "# Create a DataFrame from the lists\n",
    "df = pd.DataFrame({\n",
    "    'True_Labels': true_labels_list,\n",
    "    'Predicted_Labels': predicted_labels_list,\n",
    "    'Probabilities': probabilities_list\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor_base = AutoFeatureExtractor.from_pretrained(\"MattyB95/AST-ASVspoof2019-Synthetic-Voice-Detection-New\")\n",
    "model_base = AutoModelForAudioClassification.from_pretrained(\"MattyB95/AST-ASVspoof2019-Synthetic-Voice-Detection-New\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kk/cjf23s7n4qd7bd7fc55tzfhh0000gn/T/ipykernel_58150/2799367117.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(model_base.device)\n"
     ]
    }
   ],
   "source": [
    "model_base.eval()\n",
    "\n",
    "# Initialize lists to hold the data\n",
    "predicted_labels_list = []\n",
    "probabilities_list = []\n",
    "true_labels_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_dataloader:\n",
    "        input_values = inputs[\"input_values\"].squeeze(0)  # Remove batch dimension\n",
    "        # Move tensor to the appropriate device\n",
    "        input_values = input_values.to(model_base.device)\n",
    "        labels = torch.tensor(labels).to(model_base.device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model_base(input_values=input_values)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Apply softmax to convert logits into probabilities\n",
    "        probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "        # Get the predicted labels\n",
    "        _, predicted_labels = torch.max(probabilities, dim=-1)\n",
    "\n",
    "        # Convert tensors to list and store in the lists\n",
    "        predicted_labels_list.extend(predicted_labels.cpu().numpy())\n",
    "        probabilities_list.extend(probabilities.cpu().numpy())\n",
    "        true_labels_list.extend(labels.cpu().numpy())\n",
    "\n",
    "# Create a DataFrame from the lists\n",
    "df_base = pd.DataFrame({\n",
    "    'True_Labels': true_labels_list,\n",
    "    'Predicted_Labels': predicted_labels_list,\n",
    "    'Probabilities': probabilities_list\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor_load = AutoFeatureExtractor.from_pretrained(\"MattyB95/AST-ASVspoof2019-Synthetic-Voice-Detection-New\")\n",
    "model_load = AutoModelForAudioClassification.from_pretrained(\"MattyB95/AST-ASVspoof2019-Synthetic-Voice-Detection-New\")\n",
    "\n",
    "# Load the saved state dictionary\n",
    "checkpoint = torch.load('/Users/saji/Desktop/juliusbaer-main/notebooks/matty/best_model_epoch_8_no_scheduler_matty.pth')\n",
    "\n",
    "# Load the model state dictionary\n",
    "model_load.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kk/cjf23s7n4qd7bd7fc55tzfhh0000gn/T/ipykernel_60118/2659433406.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(model_load.device)\n"
     ]
    }
   ],
   "source": [
    "model_load.eval()\n",
    "\n",
    "# Initialize lists to hold the data\n",
    "predicted_labels_list = []\n",
    "probabilities_list = []\n",
    "true_labels_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_dataloader:\n",
    "        input_values = inputs[\"input_values\"].squeeze(0)  # Remove batch dimension\n",
    "        # Move tensor to the appropriate device\n",
    "        input_values = input_values.to(model_load.device)\n",
    "        labels = torch.tensor(labels).to(model_load.device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model_load(input_values=input_values)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Apply softmax to convert logits into probabilities\n",
    "        probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "        # Get the predicted labels\n",
    "        _, predicted_labels = torch.max(probabilities, dim=-1)\n",
    "\n",
    "        # Convert tensors to list and store in the lists\n",
    "        predicted_labels_list.extend(predicted_labels.cpu().numpy())\n",
    "        probabilities_list.extend(probabilities.cpu().numpy())\n",
    "        true_labels_list.extend(labels.cpu().numpy())\n",
    "\n",
    "# Create a DataFrame from the lists\n",
    "df_load = pd.DataFrame({\n",
    "    'True_Labels': true_labels_list,\n",
    "    'Predicted_Labels': predicted_labels_list,\n",
    "    'Probabilities': probabilities_list\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feature_extractor_load' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 20\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Create a DataFrame with columns \"file_name\" and \"file_name_without_ext\"\u001b[39;00m\n\u001b[1;32m     15\u001b[0m df_full \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile_path\u001b[39m\u001b[38;5;124m'\u001b[39m: full_paths,\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile_name_without_ext\u001b[39m\u001b[38;5;124m'\u001b[39m: file_names\n\u001b[1;32m     18\u001b[0m })\n\u001b[0;32m---> 20\u001b[0m full_data \u001b[38;5;241m=\u001b[39m FullAudioDataset(df_full, \u001b[43mfeature_extractor_load\u001b[49m)\n\u001b[1;32m     22\u001b[0m fulldataloader \u001b[38;5;241m=\u001b[39m DataLoader(full_data, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'feature_extractor_load' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_extractor_load = AutoFeatureExtractor.from_pretrained(\"MattyB95/AST-ASVspoof2019-Synthetic-Voice-Detection-New\")\n",
    "model_load = AutoModelForAudioClassification.from_pretrained(\"MattyB95/AST-ASVspoof2019-Synthetic-Voice-Detection-New\")\n",
    "\n",
    "# Load the saved state dictionary\n",
    "checkpoint = torch.load('/Users/saji/Desktop/juliusbaer-main/notebooks/matty_200/best_model.pth')\n",
    "\n",
    "# Load the model state dictionary\n",
    "model_load.load_state_dict(checkpoint['model_state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/saji/Desktop/juliusbaer-main/notebooks/audio_data/all/\"\n",
    "\n",
    "# Get all files with \".wav\" ending, including subdirectories\n",
    "full_paths = []\n",
    "file_names = []\n",
    "for root, dirs, files in os.walk(path):\n",
    "    for file in files:\n",
    "        if file.endswith('.wav'):\n",
    "            full_path = os.path.join(root, file)\n",
    "            file_name_without_ext = os.path.splitext(file)[0]\n",
    "            full_paths.append(full_path)\n",
    "            file_names.append(file_name_without_ext)\n",
    "\n",
    "# Create a DataFrame with columns \"file_name\" and \"file_name_without_ext\"\n",
    "df_full = pd.DataFrame({\n",
    "    'file_path': full_paths,\n",
    "    'file_name_without_ext': file_names\n",
    "})\n",
    "\n",
    "full_data = FullAudioDataset(df_full, feature_extractor_load)\n",
    "\n",
    "fulldataloader = DataLoader(full_data, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_load.eval()\n",
    "\n",
    "predicted_labels_list = []\n",
    "probabilities_list = []\n",
    "probabilities_list_0 = []\n",
    "probabilities_list_1 = []\n",
    "name_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, name in fulldataloader:\n",
    "        #print(name)\n",
    "        input_values = inputs[\"input_values\"].squeeze(0)  # Remove batch dimension\n",
    "        # Move tensor to the appropriate device\n",
    "        input_values = input_values.to(model_load.device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model_load(input_values=input_values)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Apply softmax to convert logits into probabilities\n",
    "        probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "        # Get the predicted labels\n",
    "        _, predicted_labels = torch.max(probabilities, dim=-1)\n",
    "\n",
    "        # Convert tensors to list and store in the lists\n",
    "        predicted_labels_list.extend(predicted_labels.cpu().numpy())\n",
    "        #print(probabilities.cpu().numpy()[0][1])\n",
    "        probabilities_list.extend(probabilities.cpu().numpy())\n",
    "        probabilities_list_0.extend([probabilities.cpu().numpy()[0][0]])\n",
    "        probabilities_list_1.extend([probabilities.cpu().numpy()[0][1]])\n",
    "\n",
    "        # name is a list of strings; extend the name_list with these names\n",
    "        name_list.extend(name)  # Assuming name is a list of strings\n",
    "\n",
    "# Create a DataFrame from the lists\n",
    "df_full = pd.DataFrame({\n",
    "    'Name': name_list,\n",
    "    'Predicted_Labels': predicted_labels_list,\n",
    "    'Probabilities': probabilities_list,\n",
    "    'Probabilities_0': probabilities_list_0,\n",
    "    'Probabilities_1': probabilities_list_1,\n",
    "})\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.to_csv('/Users/saji/Desktop/juliusbaer-main/notebooks/fake_pred_trained_model_200.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "bert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
